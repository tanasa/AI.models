{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c945869-14c9-41b9-88a6-ed0852b565d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e28b219-e610-4192-95a3-724d8ec18655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code from : https://towardsdatascience.com/how-to-create-a-graph-neural-network-in-python-61fd9b83b54e\n",
    "# was re-written with GPT4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac99e55b-8a24-493c-a8b7-d3233905a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNNs started getting popular with the introduction of the Graph Convolutional Network (GCN) which borrowed some concepts \n",
    "# from the CNNs to the graph world. The main idea from this kind of network, also known as Message-Passing Framework, \n",
    "# became the golden standard for many years in the area. \n",
    "\n",
    "# The Message-Passing framework states that, for every node in our graph, we will do two things:\n",
    "\n",
    "#    Aggregate the information from its neighbors\n",
    "#    Update the node information with the information from its previous layer and its neighbor aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd1e1a-d7a6-4547-bad3-ebbcc70162ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13b0fa9-cd3f-4b40-8940-c046a0b393bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bogdan/miniconda3/lib/python3.9/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/bogdan/miniconda3/lib/python3.9/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/home/bogdan/miniconda3/lib/python3.9/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "# import pyg_lib\n",
    "import torch_sparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5952da-cb37-434f-ba26-4d6764a739df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3919ffa2-8bc7-42b7-aed6-41db127447a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "In the following example, GNN architecture uses the SAGE layers.\n",
      "\n",
      "The SAGE (GraphSAGE) layers are a type of neural network layer used in Graph Neural Networks (GNNs). \n",
      "They were introduced in the paper \"Inductive Representation Learning on Large Graphs\" by Hamilton et al., 2017. \n",
      "\n",
      "The main idea behind GraphSAGE (Graph Sample and Aggregate) is to enable inductive learning on graph-structured data, \n",
      "where the model can generalize to unseen nodes.\n",
      "\n",
      "Key Features of SAGE Layers\n",
      "\n",
      "    Neighborhood Aggregation:\n",
      "        Each node aggregates feature information from its neighbors to update its representation.\n",
      "        The aggregation is parameterized and learned, which differentiates GraphSAGE from earlier approaches like GCN \n",
      "        (Graph Convolutional Networks).\n",
      "\n",
      "    Inductive Learning:\n",
      "        GraphSAGE can generate embeddings for nodes that were not present during training, making it suitable for dynamic graphs.\n",
      "\n",
      "    Sampling:\n",
      "        Instead of using all neighbors, GraphSAGE samples a fixed-size subset of neighbors to make the computation scalable on large graphs.\n",
      "\n",
      "    Aggregation Functions:\n",
      "        Several aggregation strategies can be used to combine neighbor features:\n",
      "            Mean Aggregator: Takes the mean of neighbor features.\n",
      "            Max Pooling Aggregator: Applies a neural network to each neighbor's features and takes the element-wise maximum.\n",
      "            LSTM Aggregator: Uses an LSTM to combine neighbor features (order-sensitive).\n",
      "            Concat Aggregator: Concatenates node features with aggregated neighbor features.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''' \n",
    "In the following example, GNN architecture uses the SAGE layers.\n",
    "\n",
    "The SAGE (GraphSAGE) layers are a type of neural network layer used in Graph Neural Networks (GNNs). \n",
    "They were introduced in the paper \"Inductive Representation Learning on Large Graphs\" by Hamilton et al., 2017. \n",
    "\n",
    "The main idea behind GraphSAGE (Graph Sample and Aggregate) is to enable inductive learning on graph-structured data, \n",
    "where the model can generalize to unseen nodes.\n",
    "\n",
    "Key Features of SAGE Layers\n",
    "\n",
    "    Neighborhood Aggregation:\n",
    "        Each node aggregates feature information from its neighbors to update its representation.\n",
    "        The aggregation is parameterized and learned, which differentiates GraphSAGE from earlier approaches like GCN \n",
    "        (Graph Convolutional Networks).\n",
    "\n",
    "    Inductive Learning:\n",
    "        GraphSAGE can generate embeddings for nodes that were not present during training, making it suitable for dynamic graphs.\n",
    "\n",
    "    Sampling:\n",
    "        Instead of using all neighbors, GraphSAGE samples a fixed-size subset of neighbors to make the computation scalable on large graphs.\n",
    "\n",
    "    Aggregation Functions:\n",
    "        Several aggregation strategies can be used to combine neighbor features:\n",
    "            Mean Aggregator: Takes the mean of neighbor features.\n",
    "            Max Pooling Aggregator: Applies a neural network to each neighbor's features and takes the element-wise maximum.\n",
    "            LSTM Aggregator: Uses an LSTM to combine neighbor features (order-sensitive).\n",
    "            Concat Aggregator: Concatenates node features with aggregated neighbor features.\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a9604a-e678-4221-8bf6-3ecc2b414da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We must define the number of in_channels of the network, this will be the number of features in our dataset. \n",
      "The out_channels is going to be the number of classes we are trying to predict. \n",
      "The hidden channels parameter is a value we can define ourselves that represents the number of hidden units.\n",
      "\n",
      "We must define the number of in_channels of the network, this will be the number of features in our dataset. \n",
      "The out_channels is going to be the number of classes we are trying to predict. \n",
      "The hidden channels parameter is a value we can define ourselves that represents the number of hidden units.\n",
      "\n",
      "We can set the number of layers of the network. \n",
      "For each hidden layer, we add a Batch Normalization layer and then we reset the parameters for every layer.\n",
      "\n",
      "We will use the ogbn-arxiv network in which each node is a Computer Science paper on the arxiv and \n",
      "each directed edge represents that a paper cited another. \n",
      "The task is to classify each node into a paper class.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "We must define the number of in_channels of the network, this will be the number of features in our dataset. \n",
    "The out_channels is going to be the number of classes we are trying to predict. \n",
    "The hidden channels parameter is a value we can define ourselves that represents the number of hidden units.\n",
    "\n",
    "We must define the number of in_channels of the network, this will be the number of features in our dataset. \n",
    "The out_channels is going to be the number of classes we are trying to predict. \n",
    "The hidden channels parameter is a value we can define ourselves that represents the number of hidden units.\n",
    "\n",
    "We can set the number of layers of the network. \n",
    "For each hidden layer, we add a Batch Normalization layer and then we reset the parameters for every layer.\n",
    "\n",
    "We will use the ogbn-arxiv network in which each node is a Computer Science paper on the arxiv and \n",
    "each directed edge represents that a paper cited another. \n",
    "The task is to classify each node into a paper class.\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5651ba8-b58f-41bd-bfcf-c8c92c50a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SAGE model\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, n_layers=2):\n",
    "        super(SAGE, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers_bn = torch.nn.ModuleList()\n",
    "        \n",
    "        if n_layers == 1:\n",
    "            self.layers.append(SAGEConv(in_channels, out_channels, normalize=False))\n",
    "        elif n_layers == 2:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        else:\n",
    "            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
    "            self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            \n",
    "            for _ in range(n_layers - 2):\n",
    "                self.layers.append(SAGEConv(hidden_channels, hidden_channels, normalize=False))\n",
    "                self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "            \n",
    "            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if len(self.layers) > 1:\n",
    "            looper = self.layers[:-1]\n",
    "        else:\n",
    "            looper = self.layers\n",
    "        \n",
    "        for i, layer in enumerate(looper):\n",
    "            x = layer(x, edge_index)\n",
    "            try:\n",
    "                x = self.layers_bn[i](x)\n",
    "            except IndexError:\n",
    "                pass\n",
    "            finally:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        if len(self.layers) > 1:\n",
    "            x = self.layers[-1](x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=-1), torch.var(x)\n",
    "\n",
    "    def inference(self, data, device):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x, edge_index = data.x.to(device), data.edge_index.to(device)\n",
    "            out, var = self.forward(x, edge_index)\n",
    "            return out.cpu(), var.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c6478d-b2db-4da8-b625-7f79618aa8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bogdan/miniconda3/lib/python3.9/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "target_dataset = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name=target_dataset, root='networks')\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be691849-8b40-4c2e-bb01-784b23948bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Object Components :\n",
    "#\n",
    "#    x:\n",
    "#        [169343, 128]: This is the node feature matrix. Each of the 169,343 nodes has a feature vector of size 128.\n",
    "#\n",
    "#    node_year:\n",
    "#        [169343, 1]: This represents an additional feature or attribute for each node, such as the year associated with the node. \n",
    "#        Each node has a single scalar value for this attribute.\n",
    "#\n",
    "#    y:\n",
    "#        [169343, 1]: This represents the labels or targets for the nodes. \n",
    "#        Each node has a single label value, which is often used for tasks like node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44081d38-c348-4e2c-bcf5-913123c382f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could define two Data Loaders to use during our training. \n",
    "# The first one will load only nodes from the training set and the second one will load all nodes on the network.\n",
    "\n",
    "# Notice that we shuffle the training data loader but not the total loader. \n",
    "# Also, the number of neighbors for the training loader is defined as the number per layer of the network.\n",
    "\n",
    "# train_loader = NeighborLoader(data, input_nodes=train_idx,\n",
    "#                              shuffle=True, num_workers=os.cpu_count() - 2,\n",
    "#                              batch_size=1024, num_neighbors=[30] * 2)\n",
    "\n",
    "# total_loader = NeighborLoader(data, input_nodes=None, num_neighbors=[-1],\n",
    "#                               batch_size=4096, shuffle=False,\n",
    "#                               num_workers=os.cpu_count() - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b29d73ec-6edc-40c8-88ef-8b607817badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test split\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train']\n",
    "valid_idx = split_idx['valid']\n",
    "test_idx = split_idx['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce590edc-fce8-44dc-ad50-58ac3d0fb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(data.x.shape[1], 256, dataset.num_classes, n_layers=2)\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10  # Number of epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e936036-5b22-4bff-b6b3-91ffbf444c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 4.0458, Train Acc: 0.0151, Val Acc: 0.0149, Test Acc: 0.0133\n",
      "Epoch 02, Loss: 4.1001, Train Acc: 0.3079, Val Acc: 0.2951, Test Acc: 0.1896\n",
      "Epoch 03, Loss: 4.0593, Train Acc: 0.2842, Val Acc: 0.3219, Test Acc: 0.2597\n",
      "Epoch 04, Loss: 3.0544, Train Acc: 0.3440, Val Acc: 0.3911, Test Acc: 0.3366\n",
      "Epoch 05, Loss: 2.9670, Train Acc: 0.3360, Val Acc: 0.3137, Test Acc: 0.3051\n",
      "Epoch 06, Loss: 2.7959, Train Acc: 0.3558, Val Acc: 0.3207, Test Acc: 0.2655\n",
      "Epoch 07, Loss: 2.5078, Train Acc: 0.4188, Val Acc: 0.4036, Test Acc: 0.3224\n",
      "Epoch 08, Loss: 2.3137, Train Acc: 0.4483, Val Acc: 0.4321, Test Acc: 0.3519\n",
      "Epoch 09, Loss: 2.2138, Train Acc: 0.4575, Val Acc: 0.4615, Test Acc: 0.4034\n",
      "Epoch 10, Loss: 2.1680, Train Acc: 0.4577, Val Acc: 0.4791, Test Acc: 0.4354\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass using train_mask\n",
    "    out, _ = model(data.x.to(device), data.edge_index.to(device))\n",
    "    loss = F.nll_loss(out[train_idx], data.y[train_idx].squeeze().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_acc = (out[train_idx].argmax(dim=1) == data.y[train_idx].squeeze().to(device)).float().mean().item()\n",
    "        val_acc = (out[valid_idx].argmax(dim=1) == data.y[valid_idx].squeeze().to(device)).float().mean().item()\n",
    "        test_acc = (out[test_idx].argmax(dim=1) == data.y[test_idx].squeeze().to(device)).float().mean().item()\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af8d4d-dea1-4001-b6f8-53dc43320d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
